{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51190f3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AI-GENERATED MEDIA DETECTION - GEN 3 AGNOSTIC MODEL (FIXED)\n",
    "# ============================================================================\n",
    "# This model detects AI-generated content across GAN, Diffusion, and \n",
    "# Autoregressive generators using multi-stream feature extraction and\n",
    "# hierarchical fusion.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchaudio\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import VideoMAEModel, VideoMAEConfig\n",
    "from torchvision.transforms import Compose, Resize, Normalize\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db4c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Temporal sampling\n",
    "    n_windows: int = 3\n",
    "    frames_per_clip: int = 16\n",
    "    min_frames: int = 48\n",
    "    \n",
    "    # Spatial sampling\n",
    "    full_res: Tuple[int, int] = (224, 224)\n",
    "    \n",
    "    # Feature dimensions\n",
    "    mae_hidden: int = 768\n",
    "    mae_compressed: int = 128\n",
    "    fft_dim: int = 256\n",
    "    fft_compressed: int = 64\n",
    "    physics_compressed: int = 32\n",
    "    audio_compressed: int = 32\n",
    "    drift_compressed: int = 16\n",
    "    \n",
    "    # Audio\n",
    "    sample_rate: int = 16000\n",
    "    n_mfcc: int = 13\n",
    "    \n",
    "    # Scene change detection\n",
    "    scene_change_threshold: float = 30.0\n",
    "    \n",
    "    # Motion detection\n",
    "    low_motion_threshold: float = 0.5\n",
    "    \n",
    "    # Transformer dimensions\n",
    "    fusion_dim: int = 128\n",
    "    n_heads: int = 4\n",
    "\n",
    "    # Training\n",
    "    batch_size: int = 8\n",
    "    lr_branches: float = 1e-4\n",
    "    lr_fusion: float = 5e-5\n",
    "    \n",
    "    checkpoint_path: Optional[str] = None\n",
    "\n",
    "CONFIG = Config()\n",
    "print(f\"Using device: {CONFIG.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c77ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UTILITY CLASSES\n",
    "# ============================================================================\n",
    "\n",
    "class SceneChangeDetector:\n",
    "    \"\"\"Detects hard cuts in video to prevent false positives in drift detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 30.0):\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def detect_cuts(self, frames: List[np.ndarray]) -> List[int]:\n",
    "        cuts = []\n",
    "        for i in range(len(frames) - 1):\n",
    "            gray1 = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
    "            gray2 = cv2.cvtColor(frames[i + 1], cv2.COLOR_BGR2GRAY)\n",
    "            diff = np.mean(np.abs(gray1.astype(float) - gray2.astype(float)))\n",
    "            if diff > self.threshold:\n",
    "                cuts.append(i + 1)\n",
    "        return cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9410e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STREAM 1: VideoMAE - Semantic Understanding (Autoregressive Detection)\n",
    "# ============================================================================\n",
    "\n",
    "class VideoMAEStream(nn.Module):\n",
    "    \"\"\"\n",
    "    Detects semantic drift and object permanence violations.\n",
    "    Best for: Autoregressive models, scene coherence\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        mae_config = VideoMAEConfig.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "        self.model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\", config=mae_config)\n",
    "        \n",
    "        # Freeze backbone to save VRAM (unfreeze for fine-tuning)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.compression = nn.Sequential(\n",
    "            nn.Linear(config.mae_hidden * config.n_windows, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, config.mae_compressed)\n",
    "        )\n",
    "        self.normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    def extract_features(self, clips: torch.Tensor, has_scene_cuts: bool = False) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clips: (n_windows, frames, 3, H, W) tensor\n",
    "        Returns:\n",
    "            Dict with embeddings, compressed features, and drift metrics\n",
    "        \"\"\"\n",
    "        n_windows = clips.shape[0]\n",
    "        embeddings = []\n",
    "        \n",
    "        with torch.no_grad():  # Backbone is frozen\n",
    "            for i in range(n_windows):\n",
    "                clip = clips[i].unsqueeze(0)\n",
    "                outputs = self.model(pixel_values=clip.to(self.config.device))\n",
    "                embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "                embedding = F.normalize(embedding, p=2, dim=1)\n",
    "                embeddings.append(embedding)\n",
    "        \n",
    "        embeddings = torch.cat(embeddings, dim=0)  # (n_windows, hidden)\n",
    "        \n",
    "        # Drift metrics (statistical, detached from gradient)\n",
    "        e = embeddings\n",
    "        drift_primary = 1 - F.cosine_similarity(e[0:1], e[-1:], dim=1).item()\n",
    "        drift_1_to_2 = 1 - F.cosine_similarity(e[0:1], e[1:2], dim=1).item()\n",
    "        drift_2_to_3 = 1 - F.cosine_similarity(e[1:2], e[2:3], dim=1).item()\n",
    "        drift_acceleration = drift_2_to_3 - drift_1_to_2\n",
    "        drift_axis_consistency = torch.var(e, dim=0).mean().item()\n",
    "        \n",
    "        if has_scene_cuts:\n",
    "            drift_primary *= 0.3\n",
    "            drift_acceleration *= 0.3\n",
    "            \n",
    "        # Compress (Gradient flows through here!)\n",
    "        flat_embeddings = embeddings.flatten().unsqueeze(0) \n",
    "        compressed = self.compression(flat_embeddings)\n",
    "        \n",
    "        return {\n",
    "            'embeddings': embeddings,  # Keep on graph for transformer\n",
    "            'compressed': compressed,\n",
    "            'drift_metrics': {\n",
    "                'drift_primary': drift_primary,\n",
    "                'drift_acceleration': drift_acceleration,\n",
    "                'drift_axis_consistency': drift_axis_consistency\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0debfffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STREAM 2: Audio Analysis (Voice Cloning / Lip-Sync Detection)\n",
    "# ============================================================================\n",
    "\n",
    "class AudioStream:\n",
    "    \"\"\"\n",
    "    Extracts MFCC features for audio artifact detection and temporal sync.\n",
    "    Best for: Voice cloners (ElevenLabs), lip-sync tools (Wav2Lip)\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.mfcc_transform = torchaudio.transforms.MFCC(\n",
    "            sample_rate=config.sample_rate,\n",
    "            n_mfcc=config.n_mfcc\n",
    "        )\n",
    "    \n",
    "    def extract_features(self, audio_waveform: Optional[torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_waveform: (channels, samples) tensor or None\n",
    "        Returns:\n",
    "            Dict with MFCC features (as tensors for gradient potential)\n",
    "        \"\"\"\n",
    "        # No audio case - return zeros\n",
    "        if audio_waveform is None or audio_waveform.shape[-1] < 1000:\n",
    "            return {\n",
    "                'mfcc_features': torch.zeros(self.config.n_mfcc * 3),\n",
    "                'temporal_features': torch.zeros((self.config.n_windows, self.config.n_mfcc)),\n",
    "                'artifact_score': 0.0,\n",
    "                'sync_score': 0.5,\n",
    "                'has_audio': 0.0\n",
    "            }\n",
    "        \n",
    "        # Convert stereo to mono\n",
    "        if audio_waveform.shape[0] > 1:\n",
    "            audio_waveform = audio_waveform.mean(dim=0, keepdim=True)\n",
    "            \n",
    "        # MFCC: output shape is (1, n_mfcc, time)\n",
    "        mfcc = self.mfcc_transform(audio_waveform) \n",
    "        \n",
    "        # FIX: Correct temporal slicing on Dimension 2 (Time), not Dimension 1 (Freq)\n",
    "        total_time = mfcc.shape[-1]\n",
    "        chunk_size = max(1, total_time // self.config.n_windows)\n",
    "        temporal_features = []\n",
    "        \n",
    "        for i in range(self.config.n_windows):\n",
    "            start = i * chunk_size\n",
    "            end = min((i + 1) * chunk_size, total_time)\n",
    "            # Slice [channel, n_mfcc, time_slice] -> mean over time\n",
    "            chunk = mfcc[:, :, start:end]\n",
    "            temporal_features.append(chunk.mean(dim=-1).squeeze(0))  # (n_mfcc,)\n",
    "            \n",
    "        temporal_features = torch.stack(temporal_features)  # (n_windows, n_mfcc)\n",
    "        \n",
    "        # Global stats for artifact detection\n",
    "        mfcc_mean = mfcc.mean(dim=-1).squeeze(0)  # (n_mfcc,)\n",
    "        mfcc_std = mfcc.std(dim=-1).squeeze(0)    # (n_mfcc,)\n",
    "        mfcc_delta = torch.diff(mfcc, dim=-1).mean(dim=-1).squeeze(0)  # (n_mfcc,)\n",
    "        mfcc_features = torch.cat([mfcc_mean, mfcc_std, mfcc_delta])\n",
    "        \n",
    "        # Spectral smoothness (synthetic audio is often \"too clean\")\n",
    "        spectral_smoothness = 1.0 - min(mfcc_std.mean().item() / 10.0, 1.0)\n",
    "        \n",
    "        return {\n",
    "            'mfcc_features': mfcc_features,\n",
    "            'temporal_features': temporal_features,\n",
    "            'artifact_score': spectral_smoothness,\n",
    "            'sync_score': 0.5,  # TODO: Replace with actual sync computation\n",
    "            'has_audio': 1.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STREAM 3: Physics Consistency (Diffusion Model Detection)\n",
    "# ============================================================================\n",
    "\n",
    "class PhysicsStream:\n",
    "    \"\"\"\n",
    "    Detects motion inconsistencies and physics violations via Optical Flow.\n",
    "    Best for: Diffusion models (Sora, SVD, Kling, Runway)\n",
    "    \n",
    "    Note: OpenCV-based, so gradients don't flow. Acts as static feature input.\n",
    "    For differentiable flow, consider RAFT integration.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "    def extract_features(self, frames: List[np.ndarray]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frames: List of grayscale frames\n",
    "        Returns:\n",
    "            Dict with physics metrics as tensors\n",
    "        \"\"\"\n",
    "        n_windows = self.config.n_windows\n",
    "        chunk_size = len(frames) // n_windows\n",
    "        \n",
    "        consistency_errors = []\n",
    "        outlier_scores = []\n",
    "        smoothness_scores = []\n",
    "        is_low_motion_flags = []\n",
    "        \n",
    "        for i in range(n_windows):\n",
    "            window = frames[i*chunk_size : (i+1)*chunk_size]\n",
    "            if len(window) < 2:\n",
    "                consistency_errors.append(0.0)\n",
    "                outlier_scores.append(0.0)\n",
    "                smoothness_scores.append(0.0)\n",
    "                is_low_motion_flags.append(1.0)\n",
    "                continue\n",
    "            \n",
    "            window_errors = []\n",
    "            magnitudes = []\n",
    "            \n",
    "            for t in range(len(window) - 1):\n",
    "                prev, curr = window[t], window[t+1]\n",
    "                # Forward-Backward Optical Flow Consistency\n",
    "                flow_fw = cv2.calcOpticalFlowFarneback(prev, curr, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                flow_bw = cv2.calcOpticalFlowFarneback(curr, prev, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                mag_sq = np.sum((flow_fw + flow_bw)**2, axis=2)\n",
    "                window_errors.append(np.sqrt(mag_sq).mean())\n",
    "                magnitudes.append(np.sqrt(np.sum(flow_fw**2, axis=2)).mean())\n",
    "            \n",
    "            consistency_errors.append(np.mean(window_errors) if window_errors else 0.0)\n",
    "            avg_mag = np.mean(magnitudes) if magnitudes else 0.0\n",
    "            smoothness_scores.append(np.var(magnitudes) if avg_mag > 0.1 else 0.0)\n",
    "            outlier_scores.append(np.sum(np.array(window_errors) > 1.0) / len(window_errors) if window_errors else 0.0)\n",
    "            is_low_motion_flags.append(float(avg_mag < self.config.low_motion_threshold))\n",
    "\n",
    "        # Pack into tensors for downstream processing\n",
    "        return {\n",
    "            'consistency_errors': torch.tensor(consistency_errors, dtype=torch.float32),\n",
    "            'outlier_scores': torch.tensor(outlier_scores, dtype=torch.float32),\n",
    "            'smoothness_scores': torch.tensor(smoothness_scores, dtype=torch.float32),\n",
    "            'motion_ranges': torch.zeros(n_windows * 2),  # Simplified placeholder\n",
    "            'physics_violation': torch.tensor([np.mean(consistency_errors)], dtype=torch.float32),\n",
    "            'low_motion_score': torch.tensor([np.mean(is_low_motion_flags)], dtype=torch.float32)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be3cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STREAM 4: Frequency Analysis (GAN Artifact Detection)\n",
    "# ============================================================================\n",
    "\n",
    "class FrequencyStream:\n",
    "    \"\"\"\n",
    "    Detects upscaling artifacts and checkerboard patterns in frequency domain.\n",
    "    Best for: GAN-generated videos (StyleGAN, ProGAN, DeepFaceLab)\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "    \n",
    "    def extract_features(self, frames: List[np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frames: List of grayscale frames\n",
    "        Returns:\n",
    "            Dict with FFT signatures (as numpy for legacy compatibility)\n",
    "        \"\"\"\n",
    "        n_windows = self.config.n_windows\n",
    "        chunk_size = len(frames) // n_windows\n",
    "        \n",
    "        signatures = []\n",
    "        artifact_scores = []\n",
    "        \n",
    "        for i in range(n_windows):\n",
    "            window = frames[i*chunk_size : (i+1)*chunk_size]\n",
    "            if not window:\n",
    "                signatures.append(np.zeros(self.config.fft_dim))\n",
    "                artifact_scores.append(0.0)\n",
    "                continue\n",
    "            \n",
    "            # Use middle keyframe\n",
    "            keyframe = window[len(window)//2]\n",
    "            keyframe_norm = keyframe.astype(float) / 255.0\n",
    "            \n",
    "            # 2D FFT\n",
    "            f = np.fft.fft2(keyframe_norm)\n",
    "            fshift = np.fft.fftshift(f)\n",
    "            magnitude = np.abs(fshift)\n",
    "            \n",
    "            # Log scale normalization\n",
    "            magnitude = np.log(magnitude + 1e-8)\n",
    "            magnitude = (magnitude - magnitude.min()) / (magnitude.max() - magnitude.min() + 1e-8)\n",
    "            \n",
    "            # Azimuthal integration (radial profile)\n",
    "            h, w = magnitude.shape\n",
    "            center = (w//2, h//2)\n",
    "            y, x = np.ogrid[:h, :w]\n",
    "            r = np.sqrt((x - center[0])**2 + (y - center[1])**2).astype(int)\n",
    "            \n",
    "            tbin = np.bincount(r.ravel(), magnitude.ravel())\n",
    "            nr = np.bincount(r.ravel())\n",
    "            radial_profile = tbin / (nr + 1e-8)\n",
    "            \n",
    "            # Normalize to fixed size\n",
    "            if len(radial_profile) > self.config.fft_dim:\n",
    "                radial_profile = radial_profile[:self.config.fft_dim]\n",
    "            else:\n",
    "                radial_profile = np.pad(radial_profile, (0, self.config.fft_dim - len(radial_profile)))\n",
    "            \n",
    "            signatures.append(radial_profile)\n",
    "            \n",
    "            # Artifact score: checkerboard detection (32-64 pixel range)\n",
    "            if len(radial_profile) >= 64:\n",
    "                checkerboard = radial_profile[32:64]\n",
    "                baseline = radial_profile[64:128].mean() if len(radial_profile) >= 128 else radial_profile.mean()\n",
    "                artifact_scores.append(np.clip((np.max(checkerboard) - baseline) / (baseline + 1e-8), 0, 1))\n",
    "            else:\n",
    "                artifact_scores.append(0.0)\n",
    "        \n",
    "        signatures = np.array(signatures)\n",
    "        artifact_scores = np.array(artifact_scores)\n",
    "        temporal_stability = np.var(signatures, axis=0).mean()\n",
    "        \n",
    "        return {\n",
    "            'signatures': signatures,\n",
    "            'artifact_scores': artifact_scores,\n",
    "            'temporal_stability': temporal_stability\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1dacfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE COMPRESSION MODULE (Gradient-Safe)\n",
    "# ============================================================================\n",
    "\n",
    "class FeatureCompression(nn.Module):\n",
    "    \"\"\"\n",
    "    Compresses multi-stream features into fixed-size vectors for fusion.\n",
    "    All operations preserve gradient flow where applicable.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # FFT Compression\n",
    "        fft_input_dim = config.fft_dim * config.n_windows + config.n_windows + 1\n",
    "        self.fft_compressor = nn.Sequential(\n",
    "            nn.Linear(fft_input_dim, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, config.fft_compressed)\n",
    "        )\n",
    "        \n",
    "        # Physics Compression\n",
    "        physics_input_dim = config.n_windows * 5 + 2\n",
    "        self.physics_compressor = nn.Sequential(\n",
    "            nn.Linear(physics_input_dim, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, config.physics_compressed)\n",
    "        )\n",
    "        \n",
    "        # Audio Compression\n",
    "        audio_input_dim = config.n_mfcc * 3 + 3\n",
    "        self.audio_compressor = nn.Sequential(\n",
    "            nn.Linear(audio_input_dim, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, config.audio_compressed)\n",
    "        )\n",
    "        \n",
    "        # Drift Compression\n",
    "        drift_input_dim = 4\n",
    "        self.drift_compressor = nn.Sequential(\n",
    "            nn.Linear(drift_input_dim, 32),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, config.drift_compressed)\n",
    "        )\n",
    "        \n",
    "        # Projections for Cross-Attention Transformer\n",
    "        self.audio_temporal_proj = nn.Linear(config.n_mfcc, config.mae_compressed)\n",
    "        self.video_temporal_proj = nn.Linear(config.mae_hidden, config.mae_compressed)\n",
    "\n",
    "    def forward(self, features: Dict) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Compress all stream features for hierarchical fusion.\"\"\"\n",
    "        dev = self.config.device\n",
    "        \n",
    "        # --- FFT ---\n",
    "        fft_sig = torch.from_numpy(features['fft']['signatures']).float().to(dev).flatten()\n",
    "        fft_scores = torch.from_numpy(features['fft']['artifact_scores']).float().to(dev)\n",
    "        fft_stab = torch.tensor([features['fft']['temporal_stability']]).float().to(dev)\n",
    "        fft_in = torch.cat([fft_sig, fft_scores, fft_stab])\n",
    "        fft_out = self.fft_compressor(fft_in.unsqueeze(0))\n",
    "        \n",
    "        # --- Physics ---\n",
    "        phy = features['physics']\n",
    "        phy_in = torch.cat([\n",
    "            phy['consistency_errors'].float().to(dev),\n",
    "            phy['outlier_scores'].float().to(dev),\n",
    "            phy['motion_ranges'].float().to(dev),\n",
    "            phy['smoothness_scores'].float().to(dev),\n",
    "            phy['physics_violation'].float().to(dev),\n",
    "            phy['low_motion_score'].float().to(dev)\n",
    "        ])\n",
    "        phy_out = self.physics_compressor(phy_in.unsqueeze(0))\n",
    "        \n",
    "        # --- Audio ---\n",
    "        aud = features['audio']\n",
    "        aud_in = torch.cat([\n",
    "            aud['mfcc_features'].float().to(dev),\n",
    "            torch.tensor([aud['artifact_score']]).float().to(dev),\n",
    "            torch.tensor([aud['sync_score']]).float().to(dev),\n",
    "            torch.tensor([aud['has_audio']]).float().to(dev)\n",
    "        ])\n",
    "        aud_out = self.audio_compressor(aud_in.unsqueeze(0))\n",
    "        \n",
    "        # --- Drift (from MAE) ---\n",
    "        mae = features['mae']\n",
    "        drift_metrics = mae['drift_metrics']\n",
    "        drift_in = torch.tensor([\n",
    "            drift_metrics['drift_primary'],\n",
    "            drift_metrics['drift_acceleration'],\n",
    "            drift_metrics['drift_axis_consistency'],\n",
    "            0.0  # has_scene_cuts placeholder\n",
    "        ]).float().to(dev)\n",
    "        drift_out = self.drift_compressor(drift_in.unsqueeze(0))\n",
    "        \n",
    "        # --- Sequence Projections for Transformer ---\n",
    "        # Audio: (n_windows, n_mfcc) -> (n_windows, mae_compressed)\n",
    "        aud_seq_in = aud['temporal_features'].float().to(dev)\n",
    "        aud_seq_emb = self.audio_temporal_proj(aud_seq_in)\n",
    "        \n",
    "        # Video: (n_windows, mae_hidden) -> (n_windows, mae_compressed)\n",
    "        vid_seq_in = mae['embeddings'].to(dev)\n",
    "        vid_seq_emb = self.video_temporal_proj(vid_seq_in)\n",
    "        \n",
    "        return {\n",
    "            'mae': mae['compressed'],\n",
    "            'fft': fft_out,\n",
    "            'physics': phy_out,\n",
    "            'audio': aud_out,\n",
    "            'drift': drift_out,\n",
    "            'sequence_video': vid_seq_emb,\n",
    "            'sequence_audio': aud_seq_emb\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3578867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HIERARCHICAL DETECTOR (Multi-Branch + Transformer Fusion)\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-layer hierarchical detection:\n",
    "    Layer 1: Specialized branch detectors (GAN, Diffusion, Autoregressive, Audio, Semantic)\n",
    "    Layer 2: Cross-modal attention fusion for final prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Layer 1: Branch Detectors (simplified for gradient stability)\n",
    "        self.gan_detector = nn.Sequential(\n",
    "            nn.Linear(config.fft_compressed, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.diffusion_detector = nn.Sequential(\n",
    "            nn.Linear(config.physics_compressed, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.autoregressive_detector = nn.Sequential(\n",
    "            nn.Linear(config.drift_compressed, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.audio_detector = nn.Sequential(\n",
    "            nn.Linear(config.audio_compressed, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.semantic_detector = nn.Sequential(\n",
    "            nn.Linear(config.mae_compressed, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Layer 2: Cross-Modal Attention (Video ← Audio)\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=config.mae_compressed,\n",
    "            num_heads=config.n_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Final Fusion MLP\n",
    "        # Input: 5 branch scores + 1 conflict + mae_compressed sync context\n",
    "        fusion_dim = 5 + 1 + config.mae_compressed\n",
    "        self.fusion_mlp = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, feats: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feats: Compressed features from FeatureCompression\n",
    "        Returns:\n",
    "            (p_fake, branch_predictions) tuple\n",
    "        \"\"\"\n",
    "        # Layer 1: Branch predictions\n",
    "        p_gan = self.gan_detector(feats['fft'])\n",
    "        p_diffusion = self.diffusion_detector(feats['physics'])\n",
    "        p_autoregressive = self.autoregressive_detector(feats['drift'])\n",
    "        p_audio = self.audio_detector(feats['audio'])\n",
    "        p_semantic = self.semantic_detector(feats['mae'])\n",
    "        \n",
    "        branch_preds = torch.cat([p_gan, p_diffusion, p_autoregressive, p_audio, p_semantic], dim=1)\n",
    "        \n",
    "        # Conflict score (high std = branches disagree = uncertainty)\n",
    "        conflict = torch.std(branch_preds, dim=1, keepdim=True)\n",
    "        \n",
    "        # Layer 2: Cross-Attention for A/V Sync\n",
    "        vid = feats['sequence_video'].unsqueeze(0)  # (1, n_windows, dim)\n",
    "        aud = feats['sequence_audio'].unsqueeze(0)  # (1, n_windows, dim)\n",
    "        attn_out, _ = self.cross_attn(query=vid, key=aud, value=aud)\n",
    "        sync_ctx = attn_out.mean(dim=1)  # (1, dim)\n",
    "        \n",
    "        # Final Fusion\n",
    "        fusion_in = torch.cat([branch_preds, conflict, sync_ctx], dim=1)\n",
    "        p_final = self.fusion_mlp(fusion_in)\n",
    "        \n",
    "        return p_final, branch_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DETECTION PIPELINE (End-to-End)\n",
    "# ============================================================================\n",
    "\n",
    "class DetectionPipeline(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete end-to-end pipeline for AI-generated media detection.\n",
    "    Handles video loading, feature extraction, and classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Feature Extraction Streams\n",
    "        self.mae_stream = VideoMAEStream(config)\n",
    "        self.audio_stream = AudioStream(config)\n",
    "        self.physics_stream = PhysicsStream(config)\n",
    "        self.freq_stream = FrequencyStream(config)\n",
    "        \n",
    "        # Neural Processing\n",
    "        self.compressor = FeatureCompression(config)\n",
    "        self.detector = HierarchicalDetector(config)\n",
    "        \n",
    "        # Utility\n",
    "        self.scene_detector = SceneChangeDetector(config.scene_change_threshold)\n",
    "        \n",
    "    def forward_one_video(self, video_path: str, audio_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Process a single video through the full pipeline.\n",
    "        \n",
    "        Args:\n",
    "            video_path: Path to video file\n",
    "            audio_path: Optional separate audio file path\n",
    "        Returns:\n",
    "            (p_fake, branch_predictions) or None if video too short\n",
    "        \"\"\"\n",
    "        # 1. LOAD VIDEO\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        \n",
    "        if len(frames) < self.config.min_frames:\n",
    "            return None  # Skip short videos\n",
    "        \n",
    "        # 2. PREPROCESS\n",
    "        frames_rgb = [cv2.cvtColor(f, cv2.COLOR_BGR2RGB) for f in frames]\n",
    "        gray_frames = [cv2.cvtColor(f, cv2.COLOR_BGR2GRAY) for f in frames]\n",
    "        \n",
    "        # Detect scene cuts\n",
    "        has_scene_cuts = len(self.scene_detector.detect_cuts(frames)) > 0\n",
    "        \n",
    "        # 3. PREPARE VideoMAE INPUT\n",
    "        total_samples = self.config.n_windows * self.config.frames_per_clip\n",
    "        indices = np.linspace(0, len(frames)-1, total_samples, dtype=int)\n",
    "        \n",
    "        mae_input = []\n",
    "        for i in range(self.config.n_windows):\n",
    "            clip_frames = []\n",
    "            for j in range(self.config.frames_per_clip):\n",
    "                idx = indices[i * self.config.frames_per_clip + j]\n",
    "                tensor = torch.from_numpy(frames_rgb[idx]).permute(2, 0, 1).float() / 255.0\n",
    "                tensor = F.interpolate(tensor.unsqueeze(0), size=self.config.full_res).squeeze(0)\n",
    "                tensor = self.mae_stream.normalize(tensor)\n",
    "                clip_frames.append(tensor)\n",
    "            mae_input.append(torch.stack(clip_frames))\n",
    "        mae_input = torch.stack(mae_input)  # (n_windows, frames, 3, H, W)\n",
    "        \n",
    "        # 4. EXTRACT FEATURES\n",
    "        f_mae = self.mae_stream.extract_features(mae_input, has_scene_cuts)\n",
    "        f_phy = self.physics_stream.extract_features(gray_frames)\n",
    "        f_frq = self.freq_stream.extract_features(gray_frames)\n",
    "        \n",
    "        # Audio extraction\n",
    "        waveform = None\n",
    "        if audio_path is None:\n",
    "            audio_path = video_path.replace('.mp4', '.wav')\n",
    "        if os.path.exists(audio_path):\n",
    "            try:\n",
    "                waveform, sr = torchaudio.load(audio_path)\n",
    "                if sr != self.config.sample_rate:\n",
    "                    resampler = torchaudio.transforms.Resample(sr, self.config.sample_rate)\n",
    "                    waveform = resampler(waveform)\n",
    "            except Exception as e:\n",
    "                print(f\"Audio load warning: {e}\")\n",
    "        f_aud = self.audio_stream.extract_features(waveform)\n",
    "        \n",
    "        # 5. FORWARD PASS (Gradient flows here!)\n",
    "        all_feats = {'mae': f_mae, 'physics': f_phy, 'fft': f_frq, 'audio': f_aud}\n",
    "        compressed = self.compressor(all_feats)\n",
    "        p_fake, branches = self.detector(compressed)\n",
    "        \n",
    "        return p_fake, branches\n",
    "    \n",
    "    def predict(self, video_path: str, audio_path: Optional[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Inference mode prediction with detailed results.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            result = self.forward_one_video(video_path, audio_path)\n",
    "            \n",
    "        if result is None:\n",
    "            return {'error': 'Video too short'}\n",
    "        \n",
    "        p_fake, branches = result\n",
    "        return {\n",
    "            'probability_fake': p_fake.item(),\n",
    "            'prediction': 'FAKE' if p_fake.item() > 0.5 else 'REAL',\n",
    "            'confidence': abs(p_fake.item() - 0.5) * 2,\n",
    "            'gan_score': branches[0, 0].item(),\n",
    "            'diffusion_score': branches[0, 1].item(),\n",
    "            'autoregressive_score': branches[0, 2].item(),\n",
    "            'audio_score': branches[0, 3].item(),\n",
    "            'semantic_score': branches[0, 4].item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d6c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING & TESTING\n",
    "# ============================================================================\n",
    "\n",
    "def create_dummy_video(path: str, n_frames: int = 60, size: Tuple[int, int] = (224, 224)):\n",
    "    \"\"\"Create a dummy video for testing.\"\"\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(path, fourcc, 30, size)\n",
    "    for i in range(n_frames):\n",
    "        # Create gradient frame for some variation\n",
    "        frame = np.zeros((size[1], size[0], 3), dtype=np.uint8)\n",
    "        frame[:, :, 0] = i * 4 % 256  # Blue channel varies\n",
    "        out.write(frame)\n",
    "    out.release()\n",
    "    print(f\"Created dummy video: {path}\")\n",
    "\n",
    "# Initialize pipeline\n",
    "print(\"=\"*70)\n",
    "print(\"AI-Generated Media Detection - Gen 3 Agnostic Model\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDevice: {CONFIG.device}\")\n",
    "print(f\"Loading VideoMAE from HuggingFace...\")\n",
    "\n",
    "pipeline = DetectionPipeline(CONFIG).to(CONFIG.device)\n",
    "print(f\"✓ Pipeline initialized\")\n",
    "print(f\"  - VideoMAE backbone: frozen\")\n",
    "print(f\"  - Trainable params: {sum(p.numel() for p in pipeline.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING DEMO\n",
    "# ============================================================================\n",
    "\n",
    "# Create test video if it doesn't exist\n",
    "test_video_path = \"dummy_train.mp4\"\n",
    "if not os.path.exists(test_video_path):\n",
    "    create_dummy_video(test_video_path)\n",
    "\n",
    "# Setup training\n",
    "optimizer = torch.optim.Adam(pipeline.parameters(), lr=CONFIG.lr_branches)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training step demo\n",
    "pipeline.train()\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING DEMO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "result = pipeline.forward_one_video(test_video_path)\n",
    "\n",
    "if result is not None:\n",
    "    p_fake, branches = result\n",
    "    label = torch.tensor([[1.0]]).to(CONFIG.device)  # Assume FAKE\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(p_fake, label)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"\\n✓ Training step completed!\")\n",
    "    print(f\"  Loss: {loss.item():.4f}\")\n",
    "    print(f\"  P(Fake): {p_fake.item():.4f}\")\n",
    "    print(f\"\\n  Branch Scores:\")\n",
    "    print(f\"    GAN:           {branches[0,0].item():.4f}\")\n",
    "    print(f\"    Diffusion:     {branches[0,1].item():.4f}\")\n",
    "    print(f\"    Autoregressive: {branches[0,2].item():.4f}\")\n",
    "    print(f\"    Audio:         {branches[0,3].item():.4f}\")\n",
    "    print(f\"    Semantic:      {branches[0,4].item():.4f}\")\n",
    "else:\n",
    "    print(\"❌ Video too short for processing\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Model ready for training on real dataset!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41131b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INFERENCE DEMO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INFERENCE DEMO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run inference\n",
    "results = pipeline.predict(test_video_path)\n",
    "\n",
    "if 'error' not in results:\n",
    "    print(f\"\\nPrediction: {results['prediction']}\")\n",
    "    print(f\"Confidence: {results['confidence']:.1%}\")\n",
    "    print(f\"P(Fake):    {results['probability_fake']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nSpecialized Detector Scores:\")\n",
    "    print(f\"  GAN Detector:           {results['gan_score']:.4f}\")\n",
    "    print(f\"  Diffusion Detector:     {results['diffusion_score']:.4f}\")\n",
    "    print(f\"  Autoregressive Detector: {results['autoregressive_score']:.4f}\")\n",
    "    print(f\"  Audio Detector:         {results['audio_score']:.4f}\")\n",
    "    print(f\"  Semantic Detector:      {results['semantic_score']:.4f}\")\n",
    "else:\n",
    "    print(f\"Error: {results['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ARCHITECTURE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "Category A (Forensic - GAN Detection):\n",
    "  └─ FrequencyStream → fft_compressor → gan_detector\n",
    "  \n",
    "Category B (Semantic/Physics - Diffusion Detection):\n",
    "  └─ VideoMAEStream → video_temporal_proj + compression → semantic_detector\n",
    "  └─ PhysicsStream → physics_compressor → diffusion_detector\n",
    "  └─ Drift metrics → drift_compressor → autoregressive_detector\n",
    "  \n",
    "Category C (Multi-Modal - Sync Detection):\n",
    "  └─ AudioStream → audio_compressor → audio_detector\n",
    "  └─ Video×Audio Cross-Attention → sync_context\n",
    "  \n",
    "Fusion:\n",
    "  └─ All branches + conflict + sync_context → fusion_mlp → P(Fake)\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
