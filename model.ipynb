{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc3601d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchaudio\n",
    "from transformers import VideoMAEModel, VideoMAEConfig\n",
    "from torchvision.transforms import Compose, Resize, Normalize\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36eee5a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Temporal sampling\n",
    "    n_windows: int = 3\n",
    "    frames_per_clip: int = 16\n",
    "    min_frames: int = 48\n",
    "    \n",
    "    # Spatial sampling\n",
    "    full_res: Tuple[int, int] = (224, 224)\n",
    "    \n",
    "    # Feature dimensions\n",
    "    mae_hidden: int = 768\n",
    "    mae_compressed: int = 128\n",
    "    fft_dim: int = 256\n",
    "    fft_compressed: int = 64\n",
    "    physics_compressed: int = 32\n",
    "    audio_compressed: int = 32\n",
    "    drift_compressed: int = 16\n",
    "    \n",
    "    # Audio\n",
    "    sample_rate: int = 16000\n",
    "    n_mfcc: int = 13\n",
    "    \n",
    "    # Scene change detection\n",
    "    scene_change_threshold: float = 30.0  # Pixel diff threshold\n",
    "    \n",
    "    # Motion detection\n",
    "    low_motion_threshold: float = 0.5  # Optical flow magnitude threshold\n",
    "    \n",
    "    # NEW: Transformer dimensions\n",
    "    fusion_dim: int = 128\n",
    "    n_heads: int = 4\n",
    "\n",
    "    # Training\n",
    "    batch_size: int = 16\n",
    "    lr_branches: float = 1e-3\n",
    "    lr_fusion: float = 5e-4\n",
    "    \n",
    "    # Model checkpoint path (for loading trained weights)\n",
    "    checkpoint_path: Optional[str] = None\n",
    "\n",
    "CONFIG = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91164172",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# SCENE CHANGE DETECTOR\n",
    "class SceneChangeDetector:\n",
    "    \"\"\"Detects hard cuts in video to prevent false positives in drift detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 30.0):\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def detect_cuts(self, frames: List[np.ndarray]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Returns indices where scene cuts occur.\n",
    "        Args:\n",
    "            frames: List of BGR frames\n",
    "        Returns:\n",
    "            List of frame indices with scene cuts\n",
    "        \"\"\"\n",
    "        cuts = []\n",
    "        for i in range(len(frames) - 1):\n",
    "            # Convert to grayscale for comparison\n",
    "            gray1 = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
    "            gray2 = cv2.cvtColor(frames[i + 1], cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Compute mean absolute difference\n",
    "            diff = np.mean(np.abs(gray1.astype(float) - gray2.astype(float)))\n",
    "            \n",
    "            if diff > self.threshold:\n",
    "                cuts.append(i + 1)\n",
    "        \n",
    "        return cuts\n",
    "\n",
    "\n",
    "# STREAM 1: VideoMAE - Semantic Understanding\n",
    "class VideoMAEStream(nn.Module):\n",
    "    \"\"\"\n",
    "    Detects semantic drift and object permanence violations.\n",
    "    Best for: Autoregressive models, scene coherence\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Load pretrained VideoMAE\n",
    "        mae_config = VideoMAEConfig.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "        self.model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\", config=mae_config)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Feature compression MLP\n",
    "        self.compression = nn.Sequential(\n",
    "            nn.Linear(config.mae_hidden * config.n_windows, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, config.mae_compressed)\n",
    "        )\n",
    "        \n",
    "        # Normalize for RGB (ImageNet stats)\n",
    "        self.normalize = Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    \n",
    "    def extract_features(self, clips: torch.Tensor, has_scene_cuts: bool = False) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            n_windows = clips.shape[0]\n",
    "            embeddings = []\n",
    "            \n",
    "            for i in range(n_windows):\n",
    "                # VideoMAE expects (batch, frames, channels, H, W)\n",
    "                clip = clips[i].unsqueeze(0)  # Already in correct format\n",
    "                outputs = self.model(pixel_values=clip.to(self.config.device))\n",
    "                \n",
    "                # Pool over sequence length\n",
    "                embedding = outputs.last_hidden_state.mean(dim=1)  # (1, hidden)\n",
    "                embedding = F.normalize(embedding, p=2, dim=1)\n",
    "                embeddings.append(embedding)\n",
    "            \n",
    "            embeddings = torch.cat(embeddings, dim=0)  # (n_windows, hidden)\n",
    "        \n",
    "        # Compute temporal drift metrics (with scene cut awareness)\n",
    "        drift_metrics = self._compute_drift(embeddings, has_scene_cuts)\n",
    "        \n",
    "        # Compress features\n",
    "        flat_embeddings = embeddings.flatten()\n",
    "        compressed = self.compression(flat_embeddings.unsqueeze(0))\n",
    "        \n",
    "        return {\n",
    "            'embeddings': embeddings.cpu(),\n",
    "            'compressed': compressed.cpu(),\n",
    "            'drift_metrics': drift_metrics\n",
    "        }\n",
    "    \n",
    "    def _compute_drift(self, embeddings: torch.Tensor, has_scene_cuts: bool) -> Dict[str, float]:\n",
    "\n",
    "        e = embeddings\n",
    "        \n",
    "        # Primary drift: early vs late\n",
    "        drift_primary = 1 - F.cosine_similarity(e[0:1], e[-1:], dim=1).item()\n",
    "        \n",
    "        # Drift acceleration\n",
    "        drift_1_to_2 = 1 - F.cosine_similarity(e[0:1], e[1:2], dim=1).item()\n",
    "        drift_2_to_3 = 1 - F.cosine_similarity(e[1:2], e[2:3], dim=1).item()\n",
    "        drift_acceleration = drift_2_to_3 - drift_1_to_2\n",
    "        \n",
    "        # PCA-based axis consistency\n",
    "        drift_axis_consistency = torch.var(e, dim=0).mean().item()\n",
    "        \n",
    "        # If scene cuts present, dampen drift signals\n",
    "        if has_scene_cuts:\n",
    "            drift_primary *= 0.3\n",
    "            drift_acceleration *= 0.3\n",
    "        \n",
    "        return {\n",
    "            'drift_primary': drift_primary,\n",
    "            'drift_acceleration': drift_acceleration,\n",
    "            'drift_axis_consistency': drift_axis_consistency,\n",
    "            'has_scene_cuts': float(has_scene_cuts)\n",
    "        }\n",
    "\n",
    "\n",
    "# STREAM 2: Frequency Analysis - GAN Artifact Detection\n",
    "class FrequencyStream:\n",
    "    \"\"\"\n",
    "    Detects upscaling artifacts and checkerboard patterns.\n",
    "    Best for: GAN-generated videos (StyleGAN, ProGAN)\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "    \n",
    "    def extract_features(self, frames: List[np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "\n",
    "        n_windows = self.config.n_windows\n",
    "        chunk_size = len(frames) // n_windows\n",
    "        \n",
    "        signatures = []\n",
    "        artifact_scores = []\n",
    "        \n",
    "        for i in range(n_windows):\n",
    "            window = frames[i*chunk_size : (i+1)*chunk_size]\n",
    "            if not window:\n",
    "                signatures.append(np.zeros(self.config.fft_dim))\n",
    "                artifact_scores.append(0.0)\n",
    "                continue\n",
    "            \n",
    "            # Use middle keyframe\n",
    "            keyframe = window[len(window)//2]\n",
    "            \n",
    "            # Normalize to [0, 1] for lighting invariance\n",
    "            keyframe_norm = keyframe.astype(float) / 255.0\n",
    "            \n",
    "            # 2D FFT\n",
    "            f = np.fft.fft2(keyframe_norm)\n",
    "            fshift = np.fft.fftshift(f)\n",
    "            magnitude = np.abs(fshift)\n",
    "            \n",
    "            # Log scale with proper normalization\n",
    "            magnitude = np.log(magnitude + 1e-8)\n",
    "            magnitude = (magnitude - magnitude.min()) / (magnitude.max() - magnitude.min() + 1e-8)\n",
    "            \n",
    "            # Azimuthal integration (radial profile)\n",
    "            h, w = magnitude.shape\n",
    "            center = (w//2, h//2)\n",
    "            y, x = np.ogrid[:h, :w]\n",
    "            r = np.sqrt((x - center[0])**2 + (y - center[1])**2).astype(int)\n",
    "            \n",
    "            tbin = np.bincount(r.ravel(), magnitude.ravel())\n",
    "            nr = np.bincount(r.ravel())\n",
    "            radial_profile = tbin / (nr + 1e-8)\n",
    "            \n",
    "            # Normalize to fixed size\n",
    "            if len(radial_profile) > self.config.fft_dim:\n",
    "                radial_profile = radial_profile[:self.config.fft_dim]\n",
    "            else:\n",
    "                radial_profile = np.pad(radial_profile, \n",
    "                                       (0, self.config.fft_dim - len(radial_profile)))\n",
    "            \n",
    "            signatures.append(radial_profile)\n",
    "            \n",
    "            # Artifact score: peak detection in checkerboard range (32-64 pixels)\n",
    "            if len(radial_profile) >= 64:\n",
    "                checkerboard_region = radial_profile[32:64]\n",
    "                baseline = radial_profile[64:128].mean() if len(radial_profile) >= 128 else radial_profile.mean()\n",
    "                artifact_score = (np.max(checkerboard_region) - baseline) / (baseline + 1e-8)\n",
    "                artifact_scores.append(np.clip(artifact_score, 0, 1))\n",
    "            else:\n",
    "                artifact_scores.append(0.0)\n",
    "        \n",
    "        signatures = np.array(signatures)\n",
    "        artifact_scores = np.array(artifact_scores)\n",
    "        \n",
    "        # Temporal stability: variance across windows\n",
    "        temporal_stability = np.var(signatures, axis=0).mean()\n",
    "        \n",
    "        return {\n",
    "            'signatures': signatures,\n",
    "            'artifact_scores': artifact_scores,\n",
    "            'temporal_stability': temporal_stability,\n",
    "            'fingerprint': signatures.mean(axis=0)\n",
    "        }\n",
    "\n",
    "\n",
    "# STREAM 3: Physics Consistency - Diffusion Detector\n",
    "class PhysicsStream:\n",
    "    \"\"\"\n",
    "    Detects motion inconsistencies and physics violations.\n",
    "    Best for: Diffusion models (Sora, SVD, Kling)\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "    \n",
    "    def extract_features(self, frames: List[np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frames: List of grayscale frames\n",
    "        Returns:\n",
    "            Dict with flow consistency and motion metrics\n",
    "        \"\"\"\n",
    "        n_windows = self.config.n_windows\n",
    "        chunk_size = len(frames) // n_windows\n",
    "        \n",
    "        consistency_errors = []\n",
    "        outlier_scores = []\n",
    "        motion_ranges = []\n",
    "        smoothness_scores = []\n",
    "        is_low_motion_flags = []\n",
    "        \n",
    "        for i in range(n_windows):\n",
    "            window = frames[i*chunk_size : (i+1)*chunk_size]\n",
    "            if len(window) < 2:\n",
    "                consistency_errors.append(0.0)\n",
    "                outlier_scores.append(0.0)\n",
    "                motion_ranges.append([0.0, 0.0])\n",
    "                smoothness_scores.append(0.0)\n",
    "                is_low_motion_flags.append(1.0)\n",
    "                continue\n",
    "            \n",
    "            window_errors = []\n",
    "            magnitudes = []\n",
    "            \n",
    "            for t in range(len(window) - 1):\n",
    "                prev, curr = window[t], window[t+1]\n",
    "                \n",
    "                # Optical flow (Farneback - can be replaced with RAFT)\n",
    "                flow_fw = cv2.calcOpticalFlowFarneback(\n",
    "                    prev, curr, None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
    "                )\n",
    "                flow_bw = cv2.calcOpticalFlowFarneback(\n",
    "                    curr, prev, None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
    "                )\n",
    "                \n",
    "                # Consistency error\n",
    "                mag_sq = np.sum((flow_fw + flow_bw)**2, axis=2)\n",
    "                error = np.sqrt(mag_sq).mean()\n",
    "                window_errors.append(error)\n",
    "                \n",
    "                # Motion magnitude\n",
    "                mag = np.sqrt(np.sum(flow_fw**2, axis=2)).mean()\n",
    "                magnitudes.append(mag)\n",
    "            \n",
    "            # Check if low motion\n",
    "            avg_motion = np.mean(magnitudes) if magnitudes else 0.0\n",
    "            is_low_motion = float(avg_motion < self.config.low_motion_threshold)\n",
    "            is_low_motion_flags.append(is_low_motion)\n",
    "            \n",
    "            # Aggregate metrics\n",
    "            consistency_errors.append(np.mean(window_errors) if window_errors else 0.0)\n",
    "            outlier_scores.append(np.sum(np.array(window_errors) > 1.0) / len(window_errors) if window_errors else 0.0)\n",
    "            \n",
    "            if magnitudes:\n",
    "                motion_ranges.append([np.min(magnitudes), np.max(magnitudes)])\n",
    "                # Smoothness: variance of magnitudes (normalized)\n",
    "                smoothness_scores.append(np.var(magnitudes) if avg_motion > 0.1 else 0.0)\n",
    "            else:\n",
    "                motion_ranges.append([0.0, 0.0])\n",
    "                smoothness_scores.append(0.0)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        consistency_errors = np.array(consistency_errors)\n",
    "        consistency_errors = np.clip(consistency_errors / 2.0, 0, 1)\n",
    "        \n",
    "        outlier_scores = np.array(outlier_scores)\n",
    "        motion_ranges = np.array(motion_ranges)\n",
    "        smoothness_scores = np.array(smoothness_scores)\n",
    "        smoothness_scores = np.clip(smoothness_scores / 10.0, 0, 1)\n",
    "        is_low_motion_flags = np.array(is_low_motion_flags)\n",
    "        \n",
    "        # Physics violation score with low-motion gating\n",
    "        avg_low_motion = is_low_motion_flags.mean()\n",
    "        weight = 1.0 - avg_low_motion  # Reduce weight if mostly static\n",
    "        \n",
    "        physics_violation = weight * (consistency_errors.mean() + \n",
    "                                      outlier_scores.mean() + \n",
    "                                      smoothness_scores.mean()) / 3.0\n",
    "        \n",
    "        return {\n",
    "            'consistency_errors': consistency_errors,\n",
    "            'outlier_scores': outlier_scores,\n",
    "            'motion_ranges': motion_ranges,\n",
    "            'smoothness_scores': smoothness_scores,\n",
    "            'physics_violation': physics_violation,\n",
    "            'low_motion_score': avg_low_motion\n",
    "        }\n",
    "\n",
    "# STREAM 4: Audio Analysis\n",
    "class AudioStream:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.mfcc_transform = torchaudio.transforms.MFCC(\n",
    "            sample_rate=config.sample_rate,\n",
    "            n_mfcc=config.n_mfcc\n",
    "        )\n",
    "    \n",
    "    def extract_features(self, audio_waveform: Optional[torch.Tensor]) -> Dict[str, np.ndarray]:\n",
    "        # If no audio, return zeros with temporal shape (n_windows, n_mfcc)\n",
    "        if audio_waveform is None or audio_waveform.shape[-1] < 1000:\n",
    "            return {\n",
    "                'mfcc_features': np.zeros(self.config.n_mfcc * 3), # Global\n",
    "                'temporal_features': np.zeros((self.config.n_windows, self.config.n_mfcc)), # NEW: Temporal\n",
    "                'artifact_score': 0.0,\n",
    "                'sync_score': 0.5,\n",
    "                'has_audio': 0.0\n",
    "            }\n",
    "        \n",
    "        if audio_waveform.shape[0] > 1:\n",
    "            audio_waveform = audio_waveform.mean(dim=0, keepdim=True)\n",
    "            \n",
    "        mfcc = self.mfcc_transform(audio_waveform) # (n_mfcc, time)\n",
    "        \n",
    "        # --- NEW: Extract Temporal Chunks ---\n",
    "        # Split audio into n_windows to match video clips\n",
    "        total_time = mfcc.shape[-1]\n",
    "        chunk_size = total_time // self.config.n_windows\n",
    "        temporal_features = []\n",
    "        \n",
    "        for i in range(self.config.n_windows):\n",
    "            # Get slice for this window\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "            chunk = mfcc[:, start:end]\n",
    "            # Average over this specific window\n",
    "            temporal_features.append(chunk.mean(dim=-1).numpy())\n",
    "            \n",
    "        temporal_features = np.stack(temporal_features) # (n_windows, n_mfcc)\n",
    "        # ------------------------------------\n",
    "\n",
    "        # Global Stats (Keep for artifact detection)\n",
    "        mfcc_mean = mfcc.mean(dim=-1)\n",
    "        mfcc_std = mfcc.std(dim=-1)\n",
    "        mfcc_delta = torch.diff(mfcc, dim=-1).mean(dim=-1)\n",
    "        mfcc_features = torch.cat([mfcc_mean, mfcc_std, mfcc_delta]).numpy()\n",
    "        \n",
    "        spectral_smoothness = 1.0 - min(mfcc_std.mean().item() / 10.0, 1.0)\n",
    "        \n",
    "        return {\n",
    "            'mfcc_features': mfcc_features,\n",
    "            'temporal_features': temporal_features, # Return the sequence!\n",
    "            'artifact_score': spectral_smoothness,\n",
    "            'sync_score': 0.5,\n",
    "            'has_audio': 1.0\n",
    "        }\n",
    "\n",
    "# FEATURE COMPRESSION & FUSION\n",
    "class FeatureCompression(nn.Module):\n",
    "    \"\"\"Per-stream feature compression MLPs.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\tself.config = config\n",
    "        \n",
    "        # FFT compression\n",
    "        fft_input_dim = config.fft_dim * config.n_windows + config.n_windows + 1\n",
    "        self.fft_compressor = nn.Sequential(\n",
    "            nn.Linear(fft_input_dim, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, config.fft_compressed)\n",
    "        )\n",
    "        \n",
    "        # Physics compression (added low_motion_score)\n",
    "        physics_input_dim = config.n_windows * 5 + 2\n",
    "        self.physics_compressor = nn.Sequential(\n",
    "            nn.Linear(physics_input_dim, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, config.physics_compressed)\n",
    "        )\n",
    "        \n",
    "        # Audio compression (added has_audio flag)\n",
    "        audio_input_dim = config.n_mfcc * 3 + 3\n",
    "        self.audio_compressor = nn.Sequential(\n",
    "            nn.Linear(audio_input_dim, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, config.audio_compressed)\n",
    "        )\n",
    "        \n",
    "        # Drift compression (added scene cut flag)\n",
    "        drift_input_dim = 4\n",
    "        self.drift_compressor = nn.Sequential(\n",
    "            nn.Linear(drift_input_dim, 32),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, config.drift_compressed)\n",
    "        )\n",
    "\n",
    "\t# NEW: Project Audio Temporal Chunks to match VideoMAE dimension (or Fusion Dim)\n",
    "        self.audio_temporal_proj = nn.Linear(config.n_mfcc, config.mae_compressed)\n",
    "        \n",
    "        # NEW: Project VideoMAE embeddings to Fusion Dim\n",
    "        self.video_temporal_proj = nn.Linear(config.mae_hidden, config.mae_compressed)\n",
    "\n",
    "    \n",
    "    def forward(self, features: Dict) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        # FFT\n",
    "        fft_input = torch.cat([\n",
    "            torch.from_numpy(features['fft']['signatures']).flatten(),\n",
    "            torch.from_numpy(features['fft']['artifact_scores']),\n",
    "            torch.tensor([features['fft']['temporal_stability']])\n",
    "        ]).float()\n",
    "        fft_compressed = self.fft_compressor(fft_input.unsqueeze(0))\n",
    "        \n",
    "        # Physics\n",
    "        physics_input = torch.cat([\n",
    "            torch.from_numpy(features['physics']['consistency_errors']),\n",
    "            torch.from_numpy(features['physics']['outlier_scores']),\n",
    "            torch.from_numpy(features['physics']['motion_ranges']).flatten(),\n",
    "            torch.from_numpy(features['physics']['smoothness_scores']),\n",
    "            torch.tensor([features['physics']['physics_violation']]),\n",
    "            torch.tensor([features['physics']['low_motion_score']])\n",
    "        ]).float()\n",
    "        physics_compressed = self.physics_compressor(physics_input.unsqueeze(0))\n",
    "        \n",
    "        # Audio\n",
    "        audio_input = torch.cat([\n",
    "            torch.from_numpy(features['audio']['mfcc_features']),\n",
    "            torch.tensor([features['audio']['artifact_score']]),\n",
    "            torch.tensor([features['audio']['sync_score']]),\n",
    "            torch.tensor([features['audio']['has_audio']])\n",
    "        ]).float()\n",
    "        audio_compressed = self.audio_compressor(audio_input.unsqueeze(0))\n",
    "        \n",
    "        # Drift\n",
    "        drift_input = torch.tensor([\n",
    "            features['mae']['drift_metrics']['drift_primary'],\n",
    "            features['mae']['drift_metrics']['drift_acceleration'],\n",
    "            features['mae']['drift_metrics']['drift_axis_consistency'],\n",
    "            features['mae']['drift_metrics']['has_scene_cuts']\n",
    "        ]).float()\n",
    "        drift_compressed = self.drift_compressor(drift_input.unsqueeze(0))\n",
    "        \n",
    "\t# Audio: (n_windows, n_mfcc) -> (n_windows, mae_compressed)\n",
    "        audio_temp = torch.from_numpy(features['audio']['temporal_features']).float().to(self.config.device)\n",
    "        audio_emb = self.audio_temporal_proj(audio_temp)\n",
    "        \n",
    "        # Video: (n_windows, mae_hidden) -> (n_windows, mae_compressed)\n",
    "        # Note: We use the raw embeddings from VideoMAEStream, not the compressed global one\n",
    "        video_emb = features['mae']['embeddings'].to(self.config.device)\n",
    "        video_emb = self.video_temporal_proj(video_emb)\n",
    "\n",
    "\t\n",
    "        return {\n",
    "            'mae': features['mae']['compressed'],\n",
    "            'fft': fft_compressed,\n",
    "            'physics': physics_compressed,\n",
    "            'audio': audio_compressed,\n",
    "            'drift': drift_compressed\n",
    "\t    'sequence_video': video_emb, \n",
    "            'sequence_audio': audio_emb\t\t\t\n",
    "        }\n",
    "\n",
    "# HIERARCHICAL DETECTOR\n",
    "class HierarchicalDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer 1: Per-stream specialized detectors\n",
    "    Layer 2: Cross-modal fusion with confidence weighting\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Layer 1: Branch detectors\n",
    "        self.gan_detector = nn.Sequential(\n",
    "            nn.Linear(config.fft_compressed, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.diffusion_detector = nn.Sequential(\n",
    "            nn.Linear(config.physics_compressed, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.autoregressive_detector = nn.Sequential(\n",
    "            nn.Linear(config.drift_compressed, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.audio_detector = nn.Sequential(\n",
    "            nn.Linear(config.audio_compressed, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.semantic_detector = nn.Sequential(\n",
    "            nn.Linear(config.mae_compressed, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Layer 2: Fusion network\n",
    "        total_dim = 5 + 1 + (config.mae_compressed + config.fft_compressed + \n",
    "                            config.physics_compressed + config.audio_compressed + \n",
    "                            config.drift_compressed)\n",
    "        \n",
    "\n",
    "\t# Attention: Query = Video, Key/Value = Audio\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=config.mae_compressed, \n",
    "            num_heads=config.n_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Final Classifier\n",
    "        # Input: All branch scores (5) + Conflict (1) + Transformer Output (mae_compressed)\n",
    "        fusion_input_dim = 6 + config.mae_compressed\n",
    "        \n",
    "        self.fusion_mlp = nn.Sequential(\n",
    "            nn.Linear(fusion_input_dim, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, compressed_features: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Hierarchical detection with interpretability.\"\"\"\n",
    "        \n",
    "        # Layer 1: Branch predictions\n",
    "        p_gan = self.gan_detector(compressed_features['fft'])\n",
    "        p_diffusion = self.diffusion_detector(compressed_features['physics'])\n",
    "        p_autoregressive = self.autoregressive_detector(compressed_features['drift'])\n",
    "        p_audio = self.audio_detector(compressed_features['audio'])\n",
    "        p_semantic = self.semantic_detector(compressed_features['mae'])\n",
    "        \n",
    "        branch_predictions = torch.cat([p_gan, p_diffusion, p_autoregressive, \n",
    "                                       p_audio, p_semantic], dim=1)\n",
    "        \n",
    "        conflict_score = torch.std(branch_predictions, dim=1, keepdim=True)\n",
    "        \n",
    "        # Layer 2: Cross-modal fusion\n",
    "\t# 2. NEW: Transformer Fusion (Sync Detection)\n",
    "        # Get sequences: (Batch=1, n_windows, dim)\n",
    "        vid_seq = compressed_features['sequence_video'].unsqueeze(0) \n",
    "        aud_seq = compressed_features['sequence_audio'].unsqueeze(0)\n",
    "        \n",
    "        # Cross Attention: \"Does Audio explain the Video?\"\n",
    "        # Output is \"Video features weighted by Audio relevance\"\n",
    "        attn_out, _ = self.cross_attn(query=vid_seq, key=aud_seq, value=aud_seq)\n",
    "        \n",
    "        # Pool the sequence (mean over windows) to get a global sync context vector\n",
    "        sync_context = attn_out.mean(dim=1) # (1, dim)\n",
    "\n",
    "\n",
    "\t# 3. Final Prediction\n",
    "\tfusion_features = torch.cat([branch_predictions, conflict_score, sync_context], dim=1)\n",
    "        p_fake = self.fusion_mlp(fusion_features)\n",
    "        \n",
    "        return {\n",
    "            'p_fake': p_fake,\n",
    "            'branch_predictions': branch_predictions,\n",
    "            'conflict_score': conflict_score\n",
    "        }\n",
    "\n",
    "# MAIN PIPELINE\n",
    "class AIGeneratedMediaDetectionPipeline:\n",
    "    \"\"\"End-to-end detection pipeline with all fixes applied.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config = CONFIG):\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize streams\n",
    "        self.mae_stream = VideoMAEStream(config).to(config.device)\n",
    "        self.freq_stream = FrequencyStream(config)\n",
    "        self.physics_stream = PhysicsStream(config)\n",
    "        self.audio_stream = AudioStream(config)\n",
    "        self.scene_detector = SceneChangeDetector(config.scene_change_threshold)\n",
    "        \n",
    "        # Initialize compression and detector\n",
    "        self.compressor = FeatureCompression(config).to(config.device)\n",
    "        self.detector = HierarchicalDetector(config).to(config.device)\n",
    "        \n",
    "        # Load trained weights if available\n",
    "        if config.checkpoint_path:\n",
    "            self.load_checkpoint(config.checkpoint_path)\n",
    "        else:\n",
    "            print(\"WARNING: No checkpoint loaded. Model has random weights.\")\n",
    "            print(\"For inference, please provide checkpoint_path in Config.\")\n",
    "    \n",
    "    def load_checkpoint(self, path: str):\n",
    "        \"\"\"Load trained model weights.\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.config.device)\n",
    "        self.compressor.load_state_dict(checkpoint['compressor'])\n",
    "        self.detector.load_state_dict(checkpoint['detector'])\n",
    "        # VideoMAE weights are already pretrained, no need to load\n",
    "        print(f\"Loaded checkpoint from {path}\")\n",
    "    \n",
    "    def save_checkpoint(self, path: str):\n",
    "        \"\"\"Save model weights.\"\"\"\n",
    "        torch.save({\n",
    "            'compressor': self.compressor.state_dict(),\n",
    "            'detector': self.detector.state_dict(),\n",
    "            'config': self.config\n",
    "        }, path)\n",
    "        print(f\"Saved checkpoint to {path}\")\n",
    "    \n",
    "    def process_video(self, video_path: str, audio_path: Optional[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a single video and return detection results.\n",
    "        \"\"\"\n",
    "        # 1. Load video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames_bgr = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames_bgr.append(frame)\n",
    "        cap.release()\n",
    "        \n",
    "        if len(frames_bgr) < self.config.min_frames:\n",
    "            raise ValueError(f\"Video too short: {len(frames_bgr)} < {self.config.min_frames}\")\n",
    "        \n",
    "        # 2. Detect scene changes\n",
    "        scene_cuts = self.scene_detector.detect_cuts(frames_bgr)\n",
    "        has_scene_cuts = len(scene_cuts) > 0\n",
    "        \n",
    "        # 3. Convert BGR to RGB for VideoMAE\n",
    "        frames_rgb = [cv2.cvtColor(f, cv2.COLOR_BGR2RGB) for f in frames_bgr]\n",
    "        \n",
    "        # 4. Temporal sampling for VideoMAE\n",
    "        total_f = len(frames_rgb)\n",
    "        indices = [\n",
    "            np.linspace(0, total_f//3, self.config.frames_per_clip, dtype=int),\n",
    "            np.linspace(total_f//3, 2*total_f//3, self.config.frames_per_clip, dtype=int),\n",
    "            np.linspace(2*total_f//3, total_f-1, self.config.frames_per_clip, dtype=int)\n",
    "        ]\n",
    "        \n",
    "        # Prepare clips for VideoMAE (RGB, normalized)\n",
    "        mae_clips = []\n",
    "        for idx in indices:\n",
    "            clip_frames = []\n",
    "            for i in idx:\n",
    "                # Convert to tensor and normalize to [0, 1]\n",
    "                frame_tensor = torch.from_numpy(frames_rgb[i]).permute(2, 0, 1).float() / 255.0\n",
    "                # Resize\n",
    "                frame_tensor = F.interpolate(\n",
    "                    frame_tensor.unsqueeze(0), \n",
    "                    size=self.config.full_res, \n",
    "                    mode='bilinear', \n",
    "                    align_corners=False\n",
    "                ).squeeze(0)\n",
    "                # Apply ImageNet normalization\n",
    "                frame_tensor = self.mae_stream.normalize(frame_tensor)\n",
    "                clip_frames.append(frame_tensor)\n",
    "            \n",
    "            mae_clips.append(torch.stack(clip_frames))  # (frames, 3, H, W)\n",
    "        \n",
    "        mae_clips = torch.stack(mae_clips)  # (n_windows, frames, 3, H, W)\n",
    "        \n",
    "        # 5. Convert to grayscale for physics/FFT\n",
    "        gray_frames = [cv2.cvtColor(f, cv2.COLOR_BGR2GRAY) for f in frames_bgr]\n",
    "        \n",
    "        # 6. Extract features from all streams\n",
    "        mae_features = self.mae_stream.extract_features(mae_clips, has_scene_cuts)\n",
    "        fft_features = self.freq_stream.extract_features(gray_frames)\n",
    "        physics_features = self.physics_stream.extract_features(gray_frames)\n",
    "        \n",
    "        # 7. Load audio\n",
    "        audio_waveform = None\n",
    "        if audio_path:\n",
    "            try:\n",
    "                audio_waveform, _ = torchaudio.load(audio_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load audio: {e}\")\n",
    "        audio_features = self.audio_stream.extract_features(audio_waveform)\n",
    "        \n",
    "        all_features = {\n",
    "            'mae': mae_features,\n",
    "            'fft': fft_features,\n",
    "            'physics': physics_features,\n",
    "            'audio': audio_features\n",
    "        }\n",
    "        \n",
    "        # 8. Compress features\n",
    "        compressed = self.compressor(all_features)\n",
    "        \n",
    "        # 9. Hierarchical detection\n",
    "        with torch.no_grad():\n",
    "            results = self.detector(compressed)\n",
    "        \n",
    "        return {\n",
    "            'probability_fake': results['p_fake'].item(),\n",
    "            'gan_score': results['branch_predictions'][0, 0].item(),\n",
    "            'diffusion_score': results['branch_predictions'][0, 1].item(),\n",
    "            'autoregressive_score': results['branch_predictions'][0, 2].item(),\n",
    "            'audio_score': results['branch_predictions'][0, 3].item(),\n",
    "            'semantic_score': results['branch_predictions'][0, 4].item(),\n",
    "            'conflict_score': results['conflict_score'].item(),\n",
    "            'prediction': 'FAKE' if results['p_fake'].item() > 0.5 else 'REAL',\n",
    "            'metadata': {\n",
    "                'scene_cuts_detected': len(scene_cuts),\n",
    "                'has_scene_cuts': has_scene_cuts,\n",
    "                'low_motion_score': physics_features['low_motion_score'],\n",
    "                'has_audio': audio_features['has_audio']\n",
    "            }\n",
    "        }\n",
    "\n",
    "# TRAINING UTILITIES\n",
    "class Trainer:\n",
    "    \"\"\"Training utilities for the detection pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline: AIGeneratedMediaDetectionPipeline, config: Config):\n",
    "        self.pipeline = pipeline\n",
    "        self.config = config\n",
    "        \n",
    "        # Separate optimizers for hierarchical training\n",
    "        self.optimizer_branches = torch.optim.Adam([\n",
    "            {'params': pipeline.mae_stream.compression.parameters()},\n",
    "            {'params': pipeline.compressor.parameters()},\n",
    "            {'params': pipeline.detector.gan_detector.parameters()},\n",
    "            {'params': pipeline.detector.diffusion_detector.parameters()},\n",
    "            {'params': pipeline.detector.autoregressive_detector.parameters()},\n",
    "            {'params': pipeline.detector.audio_detector.parameters()},\n",
    "            {'params': pipeline.detector.semantic_detector.parameters()}\n",
    "        ], lr=config.lr_branches)\n",
    "        \n",
    "        self.optimizer_fusion = torch.optim.Adam(\n",
    "            pipeline.detector.fusion.parameters(),\n",
    "            lr=config.lr_fusion\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.BCELoss()\n",
    "    \n",
    "    def train_step(self, video_paths: List[str], labels: List[int], \n",
    "                   audio_paths: Optional[List[str]] = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Single training step.\n",
    "        Args:\n",
    "            video_paths: List of video file paths\n",
    "            labels: List of binary labels (0=real, 1=fake)\n",
    "            audio_paths: Optional list of audio file paths\n",
    "        Returns:\n",
    "            Dict with loss values\n",
    "        \"\"\"\n",
    "        self.pipeline.compressor.train()\n",
    "        self.pipeline.detector.train()\n",
    "        \n",
    "        batch_loss = 0.0\n",
    "        batch_branch_losses = []\n",
    "        \n",
    "        for i, (video_path, label) in enumerate(zip(video_paths, labels)):\n",
    "            audio_path = audio_paths[i] if audio_paths else None\n",
    "            \n",
    "            # Forward pass\n",
    "            results = self.pipeline.process_video(video_path, audio_path)\n",
    "            \n",
    "            # Prepare tensors\n",
    "            label_tensor = torch.tensor([[float(label)]], device=self.config.device)\n",
    "            p_fake = torch.tensor([[results['probability_fake']]], device=self.config.device)\n",
    "            branch_preds = torch.tensor([list(results.values())[1:6]], device=self.config.device)\n",
    "            \n",
    "            # Branch losses\n",
    "            branch_loss = self.criterion(branch_preds, label_tensor.expand_as(branch_preds))\n",
    "            \n",
    "            # Fusion loss\n",
    "            fusion_loss = self.criterion(p_fake, label_tensor)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = branch_loss + fusion_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer_branches.zero_grad()\n",
    "            self.optimizer_fusion.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer_branches.step()\n",
    "            self.optimizer_fusion.step()\n",
    "            \n",
    "            batch_loss += loss.item()\n",
    "            batch_branch_losses.append(branch_loss.item())\n",
    "        \n",
    "        return {\n",
    "            'total_loss': batch_loss / len(video_paths),\n",
    "            'branch_loss': np.mean(batch_branch_losses),\n",
    "            'fusion_loss': (batch_loss - sum(batch_branch_losses)) / len(video_paths)\n",
    "        }\n",
    "\n",
    "# USAGE EXAMPLE WITH PROPER ERROR HANDLING\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"AI-Generated Media Detection Pipeline - Robust Version\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nFixes Applied:\")\n",
    "    print(\"✓ BGR→RGB conversion for VideoMAE\")\n",
    "    print(\"✓ Scene change detection to prevent false positives\")\n",
    "    print(\"✓ Low-motion gating for physics stream\")\n",
    "    print(\"✓ Improved FFT normalization for lighting invariance\")\n",
    "    print(\"✓ Checkpoint loading/saving support\")\n",
    "    print(\"✓ Training utilities included\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Configuration\n",
    "    config = Config()\n",
    "    config.checkpoint_path = None  # Set to your checkpoint path for inference\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    print(\"\\nInitializing pipeline...\")\n",
    "    pipeline = AIGeneratedMediaDetectionPipeline(config)\n",
    "    \n",
    "    # Example: Process a video\n",
    "    video_path = \"test_video.mp4\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nProcessing video: {video_path}\")\n",
    "        results = pipeline.process_video(video_path)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"DETECTION RESULTS\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"\\nPrediction: {results['prediction']}\")\n",
    "        print(f\"Confidence: {results['probability_fake']:.1%}\")\n",
    "        \n",
    "        print(\"\\n--- Specialized Detector Scores ---\")\n",
    "        print(f\"  GAN Detector:           {results['gan_score']:.3f}\")\n",
    "        print(f\"  Diffusion Detector:     {results['diffusion_score']:.3f}\")\n",
    "        print(f\"  Autoregressive Detector: {results['autoregressive_score']:.3f}\")\n",
    "        print(f\"  Audio Detector:         {results['audio_score']:.3f}\")\n",
    "        print(f\"  Semantic Detector:      {results['semantic_score']:.3f}\")\n",
    "        \n",
    "        print(\"\\n--- Meta Information ---\")\n",
    "        print(f\"  Conflict Score:         {results['conflict_score']:.3f}\")\n",
    "        print(f\"  Scene Cuts Detected:    {results['metadata']['scene_cuts_detected']}\")\n",
    "        print(f\"  Low Motion Score:       {results['metadata']['low_motion_score']:.3f}\")\n",
    "        print(f\"  Has Audio:              {'Yes' if results['metadata']['has_audio'] else 'No'}\")\n",
    "        \n",
    "        print(\"\\n--- Interpretation ---\")\n",
    "        if results['conflict_score'] > 0.3:\n",
    "            print(\"  ⚠ High disagreement between detectors - uncertain prediction\")\n",
    "        if results['metadata']['has_scene_cuts']:\n",
    "            print(\"  ℹ Scene cuts detected - drift metrics dampened\")\n",
    "        if results['metadata']['low_motion_score'] > 0.7:\n",
    "            print(\"  ℹ Low motion detected - physics metrics down-weighted\")\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nError: Video file not found: {video_path}\")\n",
    "        print(\"\\nTo use this pipeline:\")\n",
    "        print(\"1. Prepare a dataset of real and fake videos\")\n",
    "        print(\"2. Train the model using the Trainer class\")\n",
    "        print(\"3. Save the checkpoint with pipeline.save_checkpoint('model.pth')\")\n",
    "        print(\"4. Set config.checkpoint_path = 'model.pth' for inference\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during processing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Pipeline Overview:\")\n",
    "    print(\"=\" * 70)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
